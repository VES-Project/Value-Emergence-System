---
title: "1.01 信念の形式"
date: "2025-04-17"
authors: ["嶋 直紀", "松井陽太郎"]
excerpt: "信念を確率的に形式化し、知覚をサプライズ最小化のプロセスとして捉える。変分自由エネルギー最小化の枠組みを導入する。"
---

# 信念の形式

私たちの知覚は、単に外部からの刺激を受動的に取り込むプロセスではない。知覚とは、内面に生じる心象のはたらきである。こうした心象のはたらきを捉える際、心理学では構成概念（construct）を仮定する。構成概念とは、直接観察できない内面や心理過程を説明するため、便宜的に定められる理論上の概念を指す。ここでは構成概念の一例として、私たちが「現実はこうなるだろう」と予測する信念と、「世界はこうあるだろう」というア・プリオリ（先験的）な信念をもつと仮定する。

私たちは、信念に基づく予測と異なる現実が生起したとき、驚きを感じる。知覚の意味からすれば、現実は期待通りであるほどよいため、私たちは信念を絶えず更新し、現実と予測の差異を減らそうと調整すると考える。これをア・ポステリオリ（経験的）な信念の獲得として位置づける。

ここでいう「信念（belief）」とは、確率論の哲学的な解釈における二大潮流の一つ、「ある対象についての信念の度合いとして確率を捉える」という立場に基づく。そこで、はじめに「現実はこうなるだろう」と予測する信念を確率的に形式化する。

具体的には、事象$o$がどれほど起きやすいかという信念の度合いを確率$p(o)$で表す。そして、事象$o$が生起したことに対する驚き（サプライズ）の大きさを連続関数$f(p(o))$で形式的に表す。これは、サプライズの大きさ$f(\cdot)$が信念（確率）$p(o)$に依存することを意味する。また、関数の連続性はサプライズに微小な変化をもたらす確率値の変化が、ある限定的な差異に収まることを意味する。サプライズ$f(p(o))$は経験的に以下の性質を満たすと考えられる。

1. 必ず起きると確信している事象$o$の生起にはサプライズがない。つまり、$f(p(o))=f(1)=0$である。

2. 多少なりとも起きないと信じている事象$o$の生起にはサプライズがある。つまり、確率$p$の値域$[0,1)$においてサプライズは正値をとる。

3. やや特殊な場合だが、事象$o$を内包する事象$O\supset o$が生起したことを報せる通信の後に、事象$o$が生起したことを報せる通信があったとする。このときの最終的なサプライズの和は、一度で事象$o$が生起したことを知覚するサプライズと等しい。つまり、$f(p(o))=f(p(O))+f(p(o)/p(O))$が成り立つ。これはサプライズ$f$に加法法則$f(xy)=f(x)+f(y)$が成り立つことを意味する。

これらの性質を仮定して導かれる$f(x)$の微分方程式を解くと、$f(x)=c\ln x+d,~c<0$という解を得る。これより、サプライズは次のように定義されるのが一般的である。

サプライズ（Shannon's surprise）の定義　確率$p$の事象が生起したときのサプライズを

$$-\ln p, ~(0<p\leq 1)$$

　と定義する。

このように信念を確率として形式化すると、サプライズは負の対数確率として導かれる。すると、生起すると信じる度合いが小さい（すなわち確率が低い）事象ほど、生起したときにサプライズが大きくなることが、負の対数関数の単調減少性によって自然に示される。

サプライズの定式化により、「現実は期待通りであるほどよい」という価値も「サプライズ最小化」として表される。先述では、サプライズ最小化は先験的な信念を更新し、経験的な信念を獲得する過程として位置づけられると仮定した。

そこで、「世界はこうあるだろう」という先験的な信念を形式化する。具体的に、世界の潜在的（不可知）な事態$x$が成立していると信じる度合いを確率$p(x)$で表し、その事態$x$の下で事象$o$がどれほど生起しやすいか（尤度）を条件付き確率$p(o\mid x)$で表す。このとき、先に定義した確率$p(o)$は、以下のように周辺化（marginalization）した形で得られる。

$$p(o)=\int p(x)p(o\mid x)dx$$

ここで任意の関数（確率分布）$q(x)$を導入し、イェンセンの不等式（Jensen's inequality）を用いることで、サプライズ$-\ln p(o)$の上界を評価することができる。以下はその代表的な導出過程である。

$$-\ln p(o)=-\ln\int p(x)p(o\mid x)dx=-\ln\int q(x)\frac{p(x)p(o\mid x)}{q(x)}dx$$

ここで、$-\ln$は凸関数であるため、イェンセンの不等式を適用すると、

$$-\ln p(o)\leq\int q(x)\left[-\ln\frac{p(x)p(o\mid x)}{q(x)}\right]dx=\int q(x)[\ln q(x)-\ln p(x,o)]dx$$

そして、上式の右辺を汎関数

$$F[q,o]:=\mathbb E_{q(x)}[\ln q(x)-\ln p(x,o)]$$

と定義すると、

$$-\ln p(o)\leq F[q,o]$$

が得られる。すなわち、サプライズ（Shannon's surprise）$-\ln p(o)$は、汎関数$F[q,o]$の下界（あるいは$F$がサプライズの上界）となる。ここで導かれた汎関数$F$は「自由エネルギー（Free Energy）」として知られており、サプライズ最小化は、自由エネルギーを最小化する関数$q(x)$を求める問題へと帰着する。このように、関数$q$を変数として、汎関数$F$の値を最小化する問題を「変分問題（variational problem）」と呼ぶ。

さらに、自由エネルギー$F[q,o]$を以下のように変形できる。

$$F[q,o]=\mathbb E_{q(x)}[\ln q(x)-\ln p(x\mid o)]\underbrace{-\ln p(o)}_\text{Shannon's surprise}$$

ここで、ベイズの定理

$$p(x\mid o)=\frac{p(x)p(o\mid x)}{p(o)}$$

と、$p(o)$は前述の周辺尤度である。上式からは、$F[q,o]$とサプライズ$-\ln p(o)$の差は、2つの確率分布の差異を測る尺度として知られるKL情報量（Kullback–Leibler divergence）であることがわかる。

$$D_\mathrm {KL}[q(x)\Vert p(x\mid o)]=\mathbb E_{q(x)}[\ln q(x)-\ln p(x\mid o)]$$

したがって、自由エネルギー最小化とは、$D_\mathrm {KL}[q(x)\Vert p(x\mid o)]$を極力小さくするような$q(x)$を求める問題、すなわちベイズ推論（Bayesian inference）における経験的な信念確率$p(x\mid o)$に近似する問題と等価になる。

また、自由エネルギー$F[q,o]$は以下の2つの式に変形できる。

$$F[q,o]=\underbrace{-\mathbb E_{q(x)}[\ln p(o,x)]}_\text{energy}-\underbrace{\mathbb E_{q(x)}[-\ln q(x)]}_\text{Shannon's entropy}$$

$$F[q,o]=\underbrace{\mathbb E_{q(x)}[\ln q(x)-\ln p(x)]}_\text{complexity}-\underbrace{\mathbb E_{q(x)}[\ln p(o\mid x)]}_\text{accuracy}$$

第一の式は、自由エネルギーを「エネルギー（energy）」と「エントロピー（Shannon's entropy）」の項に分解して解釈する視点を提供する。ここで「エネルギー」項は、信念$q(x)$の下での世界の潜在的な事態$x$と事象$o$の結合確率$p(o,x)$の対数の期待値（の符号反転）であり、系が「どれほど尤度の高い事態をとる可能性があるか」を測る指標とみなせる。エネルギーが小さいほど、信念 $q(x)$が仮定する事態の下で事象$o$が生起しやすいことを意味する。

一方、「エントロピー」項は、系が成りうる潜在的な事態をどれだけ幅広く考慮しているかを示す指標である。このエントロピーが高いほど、信念$q$は特定の事態に偏らず、多様な蓋然性を認める「寛容な」信念になっていると言える。ここで最大エントロピー原理（maximum entropy principle）に触れておく。この原理は、「与えられた制約条件を満たす分布の中でエントロピーが最大となる分布が、最もバイアスの少ない（無駄な仮定を加えない）分布である」と主張する。自由エネルギー最小化においてエントロピー項（の符号反転）が含まれることは、バイアスを避け、可能な限りエントロピーを高く維持しつつ観測を説明しようとする最大エントロピー原理の考え方と自然に整合することを示唆している。

続いて第二式では、「複雑さ（complexity）」と「精度（accuracy）」という2つの観点から自由エネルギーを解釈する。ここで、「複雑さ」項は事前分布$p(x)$から近似分布$q(x)$がどれほど逸脱しているかを示すKL情報量$D_\mathrm{KL}[q(x)\Vert p(x)]$に対応する。言い換えれば、先験的な信念から大きく離れた経験的な信念を採用するほど、その分だけ「複雑さ」が増大することを意味する。

一方、「精度」項は、信念$q(x)$の下で観測$o$をどれだけうまく説明できるかを示す尤度（対数尤度）の期待値であり、観測データとの整合性を高めたいという要請を示している。すなわち、自由エネルギー最小化は統計的学習理論における「バイアスーバリアンス・トレードオフ」にも通じている。言い換えれば、これは、最高度の一般化と特殊な事象への最大限の適応を両立するような経験的な信念を獲得しようとする、適応的な要請を示している。

以上より、私たちの知覚プロセスは、サプライズ最小化という要請に基づき、ベイズ推論の近似である「変分自由エネルギー最小化」として形式的に記述できることがわかる。これは、私たちが「現実はこうなるだろう」という確率的な信念（予測）を抱き、その予測が外界からの経験（現象）によって覆されると驚きを感じ、その驚きを減らす方向へと信念を更新していく──こうした一連の心的プロセスを定式化したものである。この枠組みは、カール・フリストン（Karl Friston）が提唱する自由エネルギー原理（Free energy principle）と呼ばれる理論である。
